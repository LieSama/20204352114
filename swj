import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import RidgeClassifier
from sklearn.metrics import f1_score
from collections import Counter
import matplotlib.pyplot as plt

train_df = pd.read_csv('D:/机器学习数据集/train.csv', sep='\t', nrows=100)
print(train_df.head())
'''用Pandas完成数据读取的操作:
 第一列为新闻的类别，第二列为新闻的字符'''

train_df['text_len'] = train_df['text'].apply(lambda x: len(x.split(' ')))
print(train_df['text_len'].describe())
'''句子长度分析：
赛题数据中每行句子的字符使用空格进行隔开，可以直接统计单词的个数来得到每个句子的长度
对新闻句子的统计可以得出赛题给定的文本较长，每个句子平均由872个字符构成，最短的句子长度为64，最长的句子长度为7125
'''

plt.rcParams['font.sans-serif'] = ['SimHei']#解决中文显示乱码问题
_ = plt.hist(train_df['text_len'], bins=200)
plt.xlabel('句子长度')
plt.title("句子长度直方图")
plt.savefig('./text_chart_count.png')
plt.show()
#将句子长度绘制了直方图，可见大部分句子的长度都几种在2000以内

print(' ')
train_df['label'].value_counts().plot(kind='bar')
plt.title('新闻类别统计')
plt.xlabel("类别")
plt.savefig('./category.png')
plt.show()
#对数据集的类别进行分布统计，具体统计每类新闻的样本个数

all_lines = ' '.join(list(train_df['text']))
word_count = Counter(all_lines.split(" "))
word_count = sorted(word_count.items(), key=lambda d:d[1], reverse = True)
print("len(word_count): ", len(word_count))
print("word_count[0]: ", word_count[0])
print("word_count[-1]: ", word_count[-1])
'''字符分布统计:
统计每个字符出现的次数，首先可以将训练集中所有的句子进行拼接进而划分为字符，并统计每个字符的个数
从统计结果中可以看出，在训练集中总共包括2405个字，其中编号3750的字出现的次数最多，编号5034的字出现的次数最少
'''


#使用sklearn的机器学习模型完成文本分类
train_df = pd.read_csv('D:/机器学习数据集/train.csv', sep='\t', nrows=3000)
#train_df.to_csv("train.csv",sep='\t',index=False)
tfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=3000) #TF-IDF特征提取
train_test = tfidf.fit_transform(train_df['text']) #预处理数据

clf = RidgeClassifier() #岭回归分类
clf.fit(train_test[:2000], train_df['label'].values[:2000])

val_pred = clf.predict(train_test[2000:]) #预测

with open("result.csv","w")as f:
  f.write("label\n")
  for p in val_pred:
      f.write(f"{p}\n")

print("得分：",f1_score(train_df['label'].values[2000:], val_pred, average='macro'))

